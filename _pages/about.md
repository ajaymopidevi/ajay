---
layout: about
title: about
permalink: /
subtitle: Computer Science Grad Student at University of Colorado Boulder | Computer Vision Engineer at Samsung

profile:
  align: right
  image: profile_pic.jpg
  image_circular: false # crops the image to make it circular
  
news: true  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I am 2nd year Research based Master's student in Computer Science as University of Colorado Boulder. 

I am highly interested the fields of Computer Vision and Robotics, the potential of fully autonomous systems to revolutionize search and rescue operations. By equipping robots with advanced perceptual and computational capabilities, we move closer towards autonomous navigation of hazardous environments, locating survivors, thereby mitigating risks faced by human responders and saving more lives. I have gained the computer vision and machine learning foundations to work on these challenges through my coursework and research with Autonomous and Robotics Perception Group (ARPG) at CU Boulder. With my strong academic background demonstrated by an exceptional 4.0 GPA across the Fall 2022 and Spring 2023 semesters, and a proven research track record with a co-authored publication currently under review at CoRL 2023, I am confident that I can make significant contributions to this field.

With the ARPG lab, I have also been actively contributing to the field of visual language navigation. We are currently working on integrating the vast prior information inherent in Large Language Models (LLMs) e.g., GPT and make the robots understand where to go  from a user’s instruction. With this language understanding of desired location, the perception and planning algorithms can generate a trajectory that the robots can follow, making the robot completely adhere precisely to the user’s instructions which can help with rapid responses in search and rescue scenarios. My contributions to this project at ARPG have directly led to a co-authorship on a natural language-based robotics navigation paper, currently under review at CoRL .

In addition to my work in visual language navigation, I am also working on radar-based navigation and mapping to enable the perception systems to function properly even in harsh weather conditions like smoke, fog etc. These visually degraded scenarios pose serious challenges to the existing state-of-the-art navigation techniques, which are highly dependent on lidar and camera sensors. Even though radar sensors are proven to be effective in these environments, they pose additional challenges like sparse data representations and multi-path reflections. I am currently focusing my research efforts on developing advanced deep learning architectures, e.g., transformers to mitigate these challenges and enable the radar perception algorithms equally as capable as lidar perception. This research has the potential to significantly enhance the perception capabilities of the search and rescue robots to operate in low visibility scenarios caused due to dust, smoke and darkness. 

I am also working with Kasinath's lab at CU Boulder to segment the cellular structures like ribosomes, membrane, micro-tubules, filaments from the Cryo-ET tomogram. Cryo Electron Tomography combines cryogenic sample preparation, electron microscopy and computational image analysis to visualize the internal cellular structures and macro-molecules in their native state. Limiting the annotaions to {<1%>} of entire images, we used machine learning techniques like Auto-encoders and U-Net to segment these structures.

Prior to my master's, I worked at Samsung in the Advanced Multimedia Solutions Team, developing a
